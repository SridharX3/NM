{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_addition(csv_path):\n",
    "    img_features = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    img_features = img_features.assign(key=1).merge(img_features.assign(key=1), on=\"key\", suffixes=[\"_A\", \"_B\"]).drop(\"key\", axis=1)\n",
    "    img_features = img_features[img_features[\"img_id_A\"] != img_features[\"img_id_B\"]]\n",
    "    \n",
    "    return img_features\n",
    "\n",
    "def feature_subtraction(csv_path, num_features):\n",
    "    img_features = pd.read_csv(csv_path, index_col=0)\n",
    "    \n",
    "    img_features = img_features.assign(key=1).merge(img_features.assign(key=1), on=\"key\", suffixes=[\"_A\", \"_B\"]).drop(\"key\", axis=1)\n",
    "    img_features = img_features[img_features[\"img_id_A\"] != img_features[\"img_id_B\"]]\n",
    "    \n",
    "    \n",
    "    feature_columns = [ (\"f%d_%s\" % (idx, label)) for label in [\"A\", \"B\"] for idx in range(1, num_features+1) ]\n",
    "                                  \n",
    "    for idx in range(1, num_features+1):\n",
    "        img_features[\"f%d\" % idx] = np.abs(img_features[\"f%d_A\" % idx] - img_features[\"f%d_B\" % idx]) \n",
    "     \n",
    "    for label in [\"A\", \"B\"]:\n",
    "        for idx in range(1, num_features+1):\n",
    "            del img_features[\"f%d_%s\" % (idx, label)]\n",
    "    \n",
    "    return img_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "img_id_A    object\n",
       "f1_A         int64\n",
       "f2_A         int64\n",
       "f3_A         int64\n",
       "f4_A         int64\n",
       "f5_A         int64\n",
       "f6_A         int64\n",
       "f7_A         int64\n",
       "f8_A         int64\n",
       "f9_A         int64\n",
       "img_id_B    object\n",
       "f1_B         int64\n",
       "f2_B         int64\n",
       "f3_B         int64\n",
       "f4_B         int64\n",
       "f5_B         int64\n",
       "f6_B         int64\n",
       "f7_B         int64\n",
       "f8_B         int64\n",
       "f9_B         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img_features = feature_addition(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "# img_features.dtypes\n",
    "#img_features = feature_subtraction(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", 9)\n",
    "# img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_data(csv_paths):\n",
    "    labeled_data = pd.read_csv(csv_paths[0])\n",
    "    for path in csv_paths[1:]:\n",
    "        labeled_data = pd.concat([labeled_data, pd.read_csv(path)])\n",
    "    \n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_paths = [\"./HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\", \"./HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\"]\n",
    "# labeled_data = read_labeled_data(csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_writer_pairs(img_features, labeled_data):\n",
    "    # filter from all combinations of writer pairs\n",
    "    a = img_features[\"img_id_A\"] + img_features[\"img_id_B\"]\n",
    "    b = labeled_data[\"img_id_A\"] + labeled_data[\"img_id_B\"]\n",
    "    feature_set = img_features[a.isin(b)]\n",
    "    \n",
    "    feature_set = pd.merge(feature_set, labeled_data, on=[\"img_id_A\", \"img_id_B\"])\n",
    "    \n",
    "    feature_set[\"writer_A\"] = [elm[:4] for elm in feature_set[\"img_id_A\"]]\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_set = filter_writer_pairs(img_features, labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(feature_set):\n",
    "    unique = feature_set[\"writer_A\"].unique()\n",
    "    unique = np.random.permutation(unique)\n",
    "\n",
    "    # training validation and test sets split\n",
    "    tr_idx = int(0.8 * unique.shape[0])\n",
    "    tr_s = unique[:tr_idx]\n",
    "\n",
    "    training_set = feature_set.loc[feature_set[\"writer_A\"].isin(tr_s)]\n",
    "\n",
    "\n",
    "    val_idx = tr_idx + int(0.1 * unique.shape[0])\n",
    "    val_s = unique[tr_idx: val_idx]\n",
    "    validation_set = feature_set.loc[feature_set[\"writer_A\"].isin(val_s)]\n",
    "\n",
    "\n",
    "    test_s = unique[val_idx:]\n",
    "    test_set = feature_set.loc[feature_set[\"writer_A\"].isin(test_s)]\n",
    "    \n",
    "    del training_set[\"writer_A\"]\n",
    "    del validation_set[\"writer_A\"]\n",
    "    del test_set[\"writer_A\"]\n",
    "    \n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set, validation_set, test_set = train_val_test_split(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features is dynamic here\n",
    "def get_features_and_labels(dataset, num_features, feature_extraction_type):\n",
    "    if feature_extraction_type == \"addition\":\n",
    "        feature_columns = [ (\"f%d_%s\" % (idx, label)) for label in [\"A\", \"B\"] for idx in range(1, num_features+1) ]\n",
    "    elif feature_extraction_type == \"subtraction\":\n",
    "        feature_columns = [ (\"f%d\" % idx) for idx in range(1, num_features+1) ]\n",
    "    \n",
    "    y_values = dataset[\"target\"]\n",
    "    dataset = dataset.loc[:, feature_columns]\n",
    "    dataset.insert(0, \"intercept\", 1)\n",
    "    return np.array(dataset, dtype=\"uint8\"), np.array(y_values, dtype=\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_split(dataset_type, feature_extraction_type):\n",
    "\n",
    "#     img_features = feature_addition(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", 9)\n",
    "    \n",
    "    if dataset_type == \"human\":\n",
    "        csv_features_path = \"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\"\n",
    "        csv_paths = [\"./HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\", \"./HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\"]\n",
    "        num_features = 9\n",
    "    elif dataset_type == \"gsc\":\n",
    "        csv_features_path = \"./GSC-Dataset/GSC-Features-Data/GSC-Features.csv\"\n",
    "        csv_paths = [\"./GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\", \"./GSC-Dataset/GSC-Features-Data/same_pairs.csv\"]\n",
    "        num_features = 512\n",
    "        \n",
    "    if feature_extraction_type == \"addition\":\n",
    "        img_features = feature_addition(csv_features_path)\n",
    "    elif feature_extraction_type == \"subtraction\":\n",
    "        img_features = feature_subtraction(csv_features_path, num_features)\n",
    "\n",
    "    labeled_data = read_labeled_data(csv_paths)\n",
    "    \n",
    "    filtered_feature_set = filter_writer_pairs(img_features, labeled_data)\n",
    "    \n",
    "    training_set, validation_set, test_set = train_val_test_split(filtered_feature_set)\n",
    "    \n",
    "    X_train, y_train = get_features_and_labels(training_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "    X_val, y_val = get_features_and_labels(validation_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "    X_test, y_test = get_features_and_labels(test_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logistic_loss(X_train, y_train, W):\n",
    "    i = 0\n",
    "    num = X_train.shape[0] // 100\n",
    "\n",
    "    loss = 0\n",
    "    while i < X_train.shape[0]:\n",
    "        x_tr = X_train[i:i+num]\n",
    "        y_tr = y_train[i:i+num]\n",
    "        prd = np.dot(x_tr, W)\n",
    "        h_x = 1 / (1 + np.exp(-prd))\n",
    "\n",
    "        loss += (np.sum(-np.log(h_x) * y_tr - np.log(1 - h_x) * (1 - y_tr)) / (i+num))\n",
    "        i += num\n",
    "        \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"human\", feature_extraction_type=\"addition\")\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"gsc\", feature_extraction_type=\"subtraction\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"human\", feature_extraction_type=\"subtraction\")\n",
    "pd.DataFrame(X_train).to_csv(\"./foo.csv\")\n",
    "# W = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# loss = calculate_logistic_loss(X_train, y_train, W)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_descent(W, X_train,y_train, learning_rate=0.1):\n",
    "    \n",
    "#     for k in range(200):\n",
    "#         print(\"Iteration %d\" % k)\n",
    "#         x_train_chunks = np.array_split(X_train, 100)\n",
    "#         y_train_chunks = np.array_split(y_train, 100)\n",
    "        \n",
    "#         gradient = 0\n",
    "        \n",
    "    \n",
    "#         for x_train_chunk, y_train_chunk in zip(x_train_chunks, y_train_chunks):\n",
    "#             prd = np.dot(x_train_chunk, W)\n",
    "#             h_x = 1 / (1 + np.exp(-prd))    \n",
    "#             gradient += np.sum(np.dot((h_x - y_train_chunk), x_train_chunk) )\n",
    "        \n",
    "#         W = W - (learning_rate * gradient / X_train.shape[0])\n",
    "#     return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = gradient_descent(W, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = 0\n",
    "# i = 0\n",
    "\n",
    "# num = X_train.shape[0] // 50\n",
    "\n",
    "# while i < X_train.shape[0]:\n",
    "#     x_tr = X_train[i:i+num]\n",
    "#     y_tr = y_train[i:i+num]\n",
    "#     prd = np.dot(x_tr, W)    \n",
    "#     diff += np.sum(prd - y_tr)\n",
    "#     i += num \n",
    "\n",
    "\n",
    "# accuracy = 1 - (diff / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
