{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_addition(csv_path):\n",
    "    img_features = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    img_features = img_features.assign(key=1).merge(img_features.assign(key=1), on=\"key\", suffixes=[\"_A\", \"_B\"]).drop(\"key\", axis=1)\n",
    "    img_features = img_features[img_features[\"img_id_A\"] != img_features[\"img_id_B\"]]\n",
    "    \n",
    "    return img_features\n",
    "\n",
    "def feature_subtraction(csv_path, num_features):\n",
    "    img_features = pd.read_csv(csv_path, index_col=0)\n",
    "    \n",
    "    img_features = img_features.assign(key=1).merge(img_features.assign(key=1), on=\"key\", suffixes=[\"_A\", \"_B\"]).drop(\"key\", axis=1)\n",
    "    img_features = img_features[img_features[\"img_id_A\"] != img_features[\"img_id_B\"]]\n",
    "    \n",
    "    \n",
    "    feature_columns = [ (\"f%d_%s\" % (idx, label)) for label in [\"A\", \"B\"] for idx in range(1, num_features+1) ]\n",
    "                                  \n",
    "    for idx in range(1, num_features):\n",
    "        img_features[\"f%d\" % idx] = np.abs(img_features[\"f%d_A\" % idx] - img_features[\"f%d_B\" % idx]) \n",
    "     \n",
    "    for label in [\"A\", \"B\"]:\n",
    "        for idx in range(1, num_features+1):\n",
    "            del img_features[\"f%d_%s\" % (idx, label)]\n",
    "    \n",
    "    return img_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_features = feature_addition(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "\n",
    "#img_features = feature_subtraction(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", 9)\n",
    "# img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_data(csv_paths):\n",
    "    labeled_data = pd.read_csv(csv_paths[0])\n",
    "    for path in csv_paths[1:]:\n",
    "        labeled_data = pd.concat([labeled_data, pd.read_csv(path)])\n",
    "    \n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_paths = [\"./HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\", \"./HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\"]\n",
    "# labeled_data = read_labeled_data(csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_writer_pairs(img_features, labeled_data):\n",
    "    # filter from all combinations of writer pairs\n",
    "    a = img_features[\"img_id_A\"] + img_features[\"img_id_B\"]\n",
    "    b = labeled_data[\"img_id_A\"] + labeled_data[\"img_id_B\"]\n",
    "    feature_set = img_features[a.isin(b)]\n",
    "    \n",
    "    feature_set = pd.merge(feature_set, labeled_data, on=[\"img_id_A\", \"img_id_B\"])\n",
    "    \n",
    "    feature_set[\"writer_A\"] = [elm[:4] for elm in feature_set[\"img_id_A\"]]\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_set = filter_writer_pairs(img_features, labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(feature_set):\n",
    "    unique = feature_set[\"writer_A\"].unique()\n",
    "    unique = np.random.permutation(unique)\n",
    "\n",
    "    # training validation and test sets split\n",
    "    tr_idx = int(0.8 * unique.shape[0])\n",
    "    tr_s = unique[:tr_idx]\n",
    "\n",
    "    training_set = feature_set.loc[feature_set[\"writer_A\"].isin(tr_s)]\n",
    "\n",
    "\n",
    "    val_idx = tr_idx + int(0.1 * unique.shape[0])\n",
    "    val_s = unique[tr_idx: val_idx]\n",
    "    validation_set = feature_set.loc[feature_set[\"writer_A\"].isin(val_s)]\n",
    "\n",
    "\n",
    "    test_s = unique[val_idx:]\n",
    "    test_set = feature_set.loc[feature_set[\"writer_A\"].isin(test_s)]\n",
    "    \n",
    "    del training_set[\"writer_A\"]\n",
    "    del validation_set[\"writer_A\"]\n",
    "    del test_set[\"writer_A\"]\n",
    "    \n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set, validation_set, test_set = train_val_test_split(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features is dynamic here\n",
    "def get_features_and_labels(dataset, num_features, feature_extraction_type):\n",
    "    if feature_extraction_type == \"addition\":\n",
    "        feature_columns = [ (\"f%d_%s\" % (idx, label)) for label in [\"A\", \"B\"] for idx in range(1, num_features+1) ]\n",
    "    elif feature_extraction_type == \"subtraction\":\n",
    "        feature_columns = [ (\"f%d\" % idx) for idx in range(1, num_features+1) ]\n",
    "    return np.array(dataset.loc[:, feature_columns]), np.array(dataset[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237100, 9) (237100,) (29127, 9) (29127,) (27596, 9) (27596,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\socket_var\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "def get_data_split(dataset_type, feature_extraction_type):\n",
    "\n",
    "#     img_features = feature_addition(\"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", 9)\n",
    "    \n",
    "    if dataset_type == \"human\":\n",
    "        csv_features_path = \"./HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\"\n",
    "        csv_paths = [\"./HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\", \"./HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\"]\n",
    "        num_features = 9\n",
    "    elif dataset_type == \"gsc\":\n",
    "        csv_features_path = \"./GSC-Dataset/GSC-Features-Data/GSC-Features.csv\"\n",
    "        csv_paths = [\"./GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\", \"./GSC-Dataset/GSC-Features-Data/same_pairs.csv\"]\n",
    "        num_features = 512\n",
    "        \n",
    "    if feature_extraction_type == \"addition\":\n",
    "        img_features = feature_addition(csv_features_path)\n",
    "    elif feature_extraction_type == \"subtraction\":\n",
    "        img_features = feature_subtraction(csv_features_path, num_features)\n",
    "\n",
    "    labeled_data = read_labeled_data(csv_paths)\n",
    "    \n",
    "    filtered_feature_set = filter_writer_pairs(img_features, labeled_data)\n",
    "    \n",
    "    training_set, validation_set, test_set = train_val_test_split(filtered_feature_set)\n",
    "    \n",
    "    X_train, y_train = get_features_and_labels(training_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "    X_val, y_val = get_features_and_labels(validation_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "    X_test, y_test = get_features_and_labels(test_set, num_features=9, feature_extraction_type=feature_extraction_type)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"human\", feature_extraction_type=\"addition\")\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"gsc\", feature_extraction_type=\"subtraction\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data_split(dataset_type=\"human\", feature_extraction_type=\"subtraction\")\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# i = 0\n",
    "# num = X_train.shape[0] // 100\n",
    "\n",
    "# loss = 0\n",
    "# while i < X_train.shape[0]:\n",
    "#     x_tr = X_train[i:i+num]\n",
    "#     y_tr = y_train[i:i+num]\n",
    "#     W = np.ones((x_tr.shape[1], 1))\n",
    "#     prd = np.dot(x_tr, W)\n",
    "#     h_x = 1 / (1 + np.exp(-prd))\n",
    "\n",
    "# #     loss += \n",
    "#     print(np.sum(-np.log(h_x) * y_tr - np.log(1 - h_x) * (1 - y_tr)))\n",
    "#     i += num\n",
    "\n",
    "# print(loss)\n",
    "# # from scipy.sparse import csr_matrix\n",
    "\n",
    "# # feature_columns = [ (\"f%d_%s\" % (idx, label)) for label in [\"A\", \"B\"] for idx in range(1, 10) ]\n",
    "\n",
    "# # X_train, y_train = csr_matrix(training_set.loc[:, feature_columns]), csr_matrix(training_set[\"target\"])\n",
    "\n",
    "\n",
    "# # W = csr_matrix(np.ones((X_train.shape[1], 1)))\n",
    "\n",
    "# # prd = np.dot(X_train, W)\n",
    "\n",
    "# # h_x = 1 / (2 + np.expm1(-prd))\n",
    "\n",
    "\n",
    "# # # np.dot(X_train)\n",
    "# # loss = np.sum(-y_train * np.log(h_x) - (1 - y) * np.log(1 - h_x))\n",
    "# # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
